

# 决策树

## 性别分类器

### 信息熵与基尼系数

在搭建决策树时，如何衡量随机变量的不确定是至关重要的。换句话说，我们的调优完全是基于选择什么作为最优特征指标进行调整，而熵和基尼指数都是最优特征指标。

#### 信息熵的定义及相关证明

在信息论与概率统计中，熵是最基础的概念，其表示随机变量不确定的度量。设 $X$ 是一个取有限个值的离散随机变量，其概率分布为:
$$
P\left(X=x_{i}\right)=p_{i}, i=1,2, \ldots, n
$$
则随机变量 $X$ 的熵定义为:
$$
H(X)=-\sum_{i=1}^{n} p_{i} \log p_{i}
$$
为了使上式有意义，定义 $0 \log 0=0$ 。因为熵的定义只依赖于 $X$ 的分布，而与 $X$ 的取值无关，所以我们可以将熵看成是分布的函数:
$$
H(p)=-\sum_{i=1}^{n} p_{i} \log p_{i}
$$
上面说到均匀分布的熵最大，但这只是直观的感觉，并没有证明。下面利用拉格朗日乘子法进行证明。根据拉格朗日乘子的可以将 $H(p)$ 改写成:
$$
H(p)=-\sum_{i=1}^{n} p_{i} \log p_{i}+\lambda\left(\sum_{i=1}^{n} p_{i}-1\right)
$$
$H(p)$ 对每个 $p_{i}$ 求导，得到:
$$
\frac{\partial H(p)}{\partial p_{i}}=-\ln p_{i}-1+\lambda=0, i=1,2, \ldots, n
$$
由 $-\ln p_{i}-1+\lambda=0$ 可以得到 $p_{i}=e^{\lambda-1}, i=1,2, \ldots, n$

所以可知 $p_{i}$ 是只与 $\lambda$ 相关的值，每个 $p_{i}$ 应该都相等，即 $p_{1}=p_{2}=\ldots=p_{n}=\frac{1}{n}$ ，此时 $H(p)$ 取得最大值 $\log n$ 。由此可知㒀的 值域是 $[0, \log n]$ 。

#### 基尼指数的定义及相关证明

基尼指数是经典决策树CART用于分美问题时选择最优特征的指标，是信息熵中$﹣logP$ 在$P = 1$处一阶泰勒展开后的结果。假设有 $K$ 个类，样本点属于第 $k$ 类的概率为 $p k$ ，则概率分布的基尼指数定义为:
$$
G(p)=\sum_{k=1}^{K} p_{k}\left(1-p_{k}\right)=1-\sum_{k=1}^{K} p_{k}^{2}
$$
满足条件 $\sum_{k=1}^{K} p_{k}=1$

正如上面所说，基尼指数同样可以描述一个随机变量的不确定性的程度，所以可以猜测：当 $p 1=p 2=\ldots=p_{K}=\frac{1}{K}$ 时， $G(p)$ 取得最大值，此时随机变量最不确定。那么，如何进行证明? 下面给出两种方法。

方法1: 同样可以使用拉格朗日乘子法进行证明。根据拉格朗日乘子的性质，改㝍 $G(p)$ 函数为:
$$
G(p)=1-\sum_{k=1}^{K} p_{k}^{2}+\lambda\left(\sum_{k=1}^{K} p k-1\right)
$$
$G(p)$ 对每个 $p_{i}$ 求导，得到:
$$
\frac{\partial G(p)}{\partial p_{i}}=-2 p_{i}+\lambda=0, i=1,2, \ldots, K
$$
由 $-2 p_{i}+\lambda=0$ 可知 $p_{i}$ 同样只与常数 $\lambda$ 相关，所以 $p_{1}=p 2=\ldots=p_{K}=\frac{1}{K}$ $G(p)$ 的值域为 $\left[0,1-\frac{1}{K}\right]$ 。

方法2: 构造 $K$ 维空间中的两个点 $P_{1}=\left[p_{1}, p_{2}, \ldots, p_{K}\right]^{T}$和 $P_{2}=\left[\frac{1}{K}, \frac{1}{K}, \ldots, \frac{1}{K}\right]^{T}$ ，其夹角为 $\theta$ ，所以:
$$
\cos \theta=\frac{P_{1} \cdot P_{2}}{\left|P_{1}\right| \cdot\left|P_{2}\right|}=\frac{\left[p_{1}, p_{2}, \ldots, p_{K}\right] \cdot\left[\frac{1}{K}, \frac{1}{K}, \ldots, \frac{1}{K}\right]}{\sqrt{p_{1}^{2}+p_{2}^{2}+\ldots+p_{K}^{2}} \cdot \sqrt{\frac{1}{K^{2}}+\frac{1}{K^{2}}+\ldots+\frac{1}{K^{2}}}} \leq 1
$$
所以:
$$
\sum_{k=1}^{K} p_{k}^{2} \geq \frac{\left(\sum_{k=1}^{K} p_{k}\right)^{2}}{K}
$$
于是:
$$
G(p) \leq 1-\frac{\left(\sum_{k=1}^{K} p_{k}\right)^{2}}{K}=1-\frac{1}{K}
$$
等号在 $p_{1}=p_{2}=\ldots=p_{K}=\frac{1}{K}$ 时达到。

### 搜索算法对比

在建立决策树时，我们常用到许多不同的算法进行搜索构建，其中最典型的搜索算法为ID3、C4.5、CART。

由于sklearn包内自带的决策树构建算法全部是基于CART改进算法进行的，所以这里我们对比决策树构建算法的区别，手动写了各种算法的构建代码，并基于各个算法进行比较：

####　ID3

ID3 (Iterative Dichotomiser 3) 是由 Ross Quinlan 于 1986 年开发的。该算法创建了一个多路树，为每个节点（即以贪婪的方式）找到将为分类目标产生最大信息增益的分类特征。树生长到它们的最大大小，然后通常应用修剪步骤来提高树泛化到看不见的数据的能力。

ID3 算法具体步骤：

> 输入: 训练数据集 $D$, 特征集 $A$, 阈值 $\varepsilon$;
>
> 输出: 决策树 $T$.
>
> (1) 若 $D$ 中所有实例属于同一类 $C_{k}$，则 $T$ 为单结点树，并将类 $C_{k}$ 作为该结点的类标记，返回 $T$ ；
>
> (2) 若 $A=\varnothing$，则 $T$ 为单结点树，并将 $D$ 中实例数最大的类 $C_{k}$ 作为该结点的类标记，返回 $T$ ；
>
> (3) 否则，按算法计算 $A$ 中各特征对 $D$ 的信息增益，选择信息增益最大的特征 $A_{8}$ ；
>
> (4) 如果 $A_{g}$ 的信息增益小于呵值 $\varepsilon$，则置 $T$ 为单结点树，并将 $D$ 中实例数最 大的类 $C_{k}$ 作为该结点的类标记，返回 $T$ ;
>
> (5) 否则, 对 $A_{g}$ 的每一可能值 $a_{i}$，依 $A_{g}=a_{i}$ 将 $D$ 分割为若干非空子集 $D_{i}$, 将 $D_{i}$ 中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树 $T$, 返回 $T$ ;
>
> (6) 对第 $i$ 个子结点, 以 $D_{i}$ 为训练集, 以 $A-\left\{A_{g}\right\}$ 为特征集, 递归地调用 步 (1) 〜步 (5), 得到子树 $T_{i}$, 返回 $T_{i}$.

其中数据集的信息增益计算方法为：

(1) 计算数据集 $D$ 的经验熵 $H(D)$
$$
H(D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|}{|D|} \log _{2} \frac{\left|C_{k}\right|}{|D|}
$$
(2) 计算特征 $A$ 对数据集 $D$ 的经验条件熵 $H(D \mid A)$
$$
H(D \mid A)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \sum_{k=1}^{K} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|} \log _{2} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|}
$$
(3) 计算信息增益
$$
g(D, A)=H(D)-H(D \mid A)
$$
由于ID3基础算法对无关紧要的特征也会精确分析，其类似穷举式的进行树搜索，最终建立的树效果较差，如下图所示。最终在我们的测试集上其分类正确率为 92.05%，且测试方差较大，鲁棒性较差。

![res_16_15_43](.\res_id3.jpg)

#### C4.5

C4.5 是 ID3 的后继者，并通过动态定义将连续属性值划分为一组离散区间的离散属性（基于数值变量）取消了特征必须是分类的限制。C4.5 将训练好的树（即 ID3 算法的输出）转换为 if-then 规则集。然后评估每个规则的这些准确性，以确定它们的应用顺序。如果没有规则的准确性提高，则通过删除规则的先决条件来完成修剪。

该算法建立的树在测试集上的分类正确率为 96.86%

![res_16_02_30](.\res_c45.jpg)

#### CART

CART（分类和回归树）与 C4.5 非常相似，但不同之处在于它支持数值目标变量（回归）并且不计算规则集。CART 使用在每个节点产生最大信息增益的特征和阈值构建二叉树。

CART 算法具体步骤：

> 输入: 训练数据集 $D$, 停止计算的条件;
>
> 输出: CART 决策树.
>
> 根据训练数据集, 从根结点开始, 递归地对每个结点进行以下操作, 构建二叉决策树:
>
> (1) 设结点的训练数据集为 $D$, 计算现有特征对该数据集的基尼指数. 此时, 对每一个特征 $A$, 对其可能取的每个值 $a$, 根据样本点对 $A=a$ 的测试为 “是” 或 “否” 将 $D$ 分割成 $D_{1}$ 和 $D_{2}$ 两部分, 然后计算 $A=a$ 时的基尼指数.
>
> (2) 在所有可能的特征 $A$ 以及它们所有可能的切分点 $a$ 中, 选择基尼指数最 小的特征及其对应的切分点作为最优特征与最优切分点. 依最优特征与最优切分 点, 从现结点生成两个子结点, 将训练数据集依特征分配到两个子结点中去.
>
> (3) 对两个子结点递归地调用 (1), (2), 直至满足停止条件.
>
> (4) 生成 CART 决策树.
>
> 算法停止计算的条件是结点中的样本个数小于预定阈值, 或样本集的基尼指 数小于预定间值 (样本基本属于同一类), 或者没有更多特征.

其中如果样本集合 $D$ 根据特征 $A$ 是否取某一可能值 $a$ 被分割成 $D_{1}$ 和 $D_{2}$ 两部分, 即
$$
D_{1}=\{(x, y) \in D \mid A(x)=a\}, \quad D_{2}=D-D_{1}
$$
则在特征 $A$ 的条件下, 集合 $D$ 的基尼指数定义为
$$
\operatorname{Gini}(D, A)=\frac{\left|D_{1}\right|}{|D|} \operatorname{Gini}\left(D_{1}\right)+\frac{\left|D_{2}\right|}{|D|} \operatorname{Gini}\left(D_{2}\right)
$$
该算法建立的树在测试集上的分类正确率为 97.23%。

![res_23_09_34](.\res_cart.jpg)

#### 改进CART

按照sklearn的文档介绍，其树搜索算法是基于改进CART，但并不明确其具体改进测试，推测其应该较原始CART算法改进较小。经测试，其预测准确率均值为97.50%，其分类结果和准确率于我们自己写的CART较为接近。

![tree](.\res_prove_cart.png)

为了测试的方便与结果的美观，我们的后续训练均基于sklearn的决策树包进行。

### 决策树参数训练

#### 决策树划分标准（criterion）

如前所述，决策树的划分标准（不纯度度量）指标有信息熵"Entrophy"和基尼指数”Gini“，作为最基础的参数，我们首先需要进行对比，其结果如下：

| 不划分标准 | 交叉验证结果 |
| :--------: | :----------: |
|   信息熵   |    95.54%    |
|  基尼指数  |    96.15%    |

对比发现，其它参数固定的情况下，基尼指数交叉验证结果更好，其作为我们决策树的划分标准更加合适。

#### 最大深度（Maximum Depth）

第二个参数我们修正树的最大深度，是因为模型得分一般会随着最大深度单调递增，最终趋于稳定。而且最大深度一般对模型预测准确度的影响是最大的。

首先，我们将最大深度设置为2~50，然后进行5次5折交叉验证，对每次验证的深度-性能指标/预测得分进行绘制，得到了下图的结果。可以看出，随着深度的增加，各个指标的确是如预期变化的。

![Fig33333](.\Fig_max_depth.png)

然后我们选择深度为10左右，再进行细化观察：

![Fig33](.\Fig_max_depth2.png)

经过多次实验发现，验证结果的变化与设想有一定差距。分析结果，推测最大的可能性就是各个特征对性别推算的影响差距较大，很多特征在我们性别决策树的推测中不起作用甚至会起到负面作用，所以在寻优时需要适当丢弃一些特征进行判断，树的深度并不是越大越好。

最终，我们选择4作为我们树的最大搜索深度。

#### 最小叶节点分割样本数（Minimum Samples Split）

一个节点必须至少有Minimum Samples Split个样本才能足够大以进行拆分。如果一个节点的样本数少于这个数值， 则分割过程停止，该节点不会被分割。

但是，这个数值不会控制叶的最小尺寸。例如，设置最小分割样本为11，假如父节点有20个样本，大于11，因此这个节点被拆分。但在此节点被拆分后，出现了一个子节点的样本数为5，小于11。

![image-20211214002522679](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20211214002522679.png)

我们将最小叶节点分割样本数设置为2~15，然后进行5次5折交叉验证，对每次验证的最小叶节点分割样本数-性能指标/预测得分进行绘制，得到了下图的结果。

![Figur1231231](.\Fig_min_samples_split.png)

经过验证，我们最终选择3作为最小叶节点分割样本数。

#### 最小叶节点样例数（Minimum Samples Leaf）

一个节点在分支后的每个子节点都必须包含至少Minimum Samples leaf个训练样本，这个结点才允许被分支，否则分支就不会发生。

和上面同理，我们先固定划分标准为Gini，最大深度为4，最小结点分割样本数为5进行参数5次5折交叉检验首先在0~100里查找：

![Figur1231231](.\Fig_min_samples_leaf1.png)

然后我们再在0~15里进行比较，最终选择3作为最小节点样例数。

![Figur1231231](.\Fig_min_samples_leaf2.png)

#### 改进效果对比

我们将改进过的参数与为改进前对比，选用5*5折交叉验证，比较准确率，得到如下结果：

改进前：平均准确率约为95.2%。

![Fig_prove](.\Fig_unprove.png)

改进后：平均准确率约为97.1%。

![Fig_prove](.\Fig_prove.png)

最终参数优化后生成的树为：

![tree](.\tree.png)

### 决策树的剪枝

#### 理论分析

决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现。设树 $T$ 的叶结点个数为 $|T|$ ，$t$ 是树 $T$ 的叶结点，该叶结点有 $N_{t}$ 个样本点，其中 $k$ 类的样本点有 $N_{t k}$ 个, $k=1,2, \cdots, K$  ，$H_{t}(T)$为叶结点 $t$ 上的经验熵， $\alpha \geqslant 0$ 为参数, 则决策树学习的损失函数可以定义为：
$$
C_{\alpha}(T)=\sum_{t=1}^{|T|} N_{t} H_{t}(T)+\alpha|T|
$$
其中经验熵为：
$$
H_{t}(T)=-\sum_{k} \frac{N_{tk}}{N_{t}} \log \frac{N_{tk}}{N_{t}}
$$
在损失函数中, 将式 (4.1) 右端的第 1 项记作：
$$
C(T)=\sum_{t=1}^{|T|} N_{t} H_{t}(T)=-\sum_{t=1}^{|T|} \sum_{k=1}^{K} N_{tk} \log \frac{N_{tk}}{N_{t}}
$$
这时有：
$$
C_{\alpha}(T)=C(T)+\alpha|T|
$$
式 中, $C(T)$ 表示模型对训练数据的预测误差, 即模型与训练数据的拟合程度， $|T|$ 表示模型复杂度，参数 $\alpha \geqslant 0$ 控制两者之间的影响。 较大的 $\alpha$ 促使选择较筒单的模型（树），较小的 $\alpha$ 促使选择较复杂的模型（树），\alpha=0$ 意味着只考虑模型与训练数据的拟合程度, 不考虑模型的复杂度。

剪枝, 就是当 $\alpha$ 确定时，选择损失函数最小的模型，即损失函数最小的子树。当 $\alpha$ 值确定时，子树越大，往往与训练数据的拟合越好，但是模型的复杂度就越高；相反，子树越小，模型的复杂度就越低，但是往往与训练数据的拟合不好。损失函数正好表示了对两者的平衡。

可以看出，决策树生成只考虑了通过提高信息增益（或信息增益比）对训练数据进行更好的拟合。而决策树剪枝通过优化损失函数还考虑了减小模型复杂度。决策树生成学习局部的模型, 而决策树剪枝学习整体的模型。

#### 寻找有效 $a$ 值

最小成本复杂度修剪递归地找到具有“最弱链接”的节点。最弱的链接以有效 $a$ 为特征，其中具有最小有效 $a$ 的节点首先被修剪。为了了解哪些可能的 $a$ 值是合适的，sklearn 包提供了在修剪过程的每个步骤中返回有效 $a$ 值和相应的总叶子杂质的方法。随着 $a$ 的增加，更多的树被修剪，这增加了其叶子的总杂质。

在下图中，最大有效 $a$ 值被删除，因为它是只有一个节点的平凡树。

![pruning_1](.\pruning_1.png)

接下来，我们使用有效 $a$ 训练决策树。在这里，我们展示了节点数量和树深度随着 alpha 的增加而减少。

![pruning_2](.\pruning_2.png)

当 $a$ 设置为零并保持其他默认参数时，树会过度拟合，导致 100% 的训练准确度和 88% 的测试准确度。随着 alpha 的增加，更多的树被修剪，从而创建一个更好泛化的决策树。在本例中，设置 $a=0.005$ 时可以最大化测试精度。

![pruning_3](.\pruning_3.png)

## 籍贯分类器

### 数据预处理

由于籍贯是新的分类量，原始数据存在部分错误的”脏“数据，还有不同标准情况下填写的籍贯信息。在分类的第一步对所有的籍贯信息进行整理是十分有必要的。

#### 地理信息提取

我们按照以下步骤进行处理：

1. 原始数据提取并转换成numpy格式

2. 调用接口，建立”中国地理信息库“

   ​        数据转换后，需要提供一个标准数据库用以检索筛选。这里我们使用高德地图开发者API，考虑数据库规模以及实际填表情况，我们只调用全中国所有市级及以上行政单位信息。接口返回JSON格式信息，如下图所示。

   ![image-20211213145930792](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20211213145930792.png)

   ​        我们对JSON数据进行提取，并按照中文习惯重新构建地理字典。在中文里，我们习惯将籍贯信息后面的省、市、自治区、特别行政区等进行简写，如”江苏省南京市“我们一般直接写成”江苏南京“。所以我们的字典里将相关信息删去，这样构建字典不仅可以为后面正则匹配提高准确率，还可以减小数据库规模。我们将构建好的数据库以JSON格式文件保存在本地，这样下次调用就无需再请求API。

3. 正则匹配

   ​        数据库建立完成后，我们就可以在进行查找匹配了。我们将数据库以字典形式导入，字典键值分别为市级单位及其对应的省级单位。我们采用二次全局正则匹配的方式进行匹配：即第一次以省级单位作为模板对待匹配籍贯进行查找，若检索到则对原始值进行标准替换；否则再以市级单位进行查找，若检索到则直接对原始值以库内键值对应标准省级名称进行替换；若两次检索均失败，则抛出错误，进行人工手动标注。

   ​        最终经过匹配，在前期处理过后的437条数据中，共有432条数据匹配成功，5条匹配失败。这5条数据分别为：

   | 原始序号 | 原始籍贯信息 |
   | :------: | :----------: |
   |    9     |     重亲     |
   |   212    |     寿光     |
   |   215    |     吴浪     |
   |   216    |     贺星     |
   |   341    |    雷皓云    |
   
   ​        可以看到，这5条数据的匹配失败大致缘于3个原因：第9条数据将重庆误写成“重亲”；第212条数据“寿光”为山东潍坊市县级行政单位；第215、216、341条数据不存在中国的行政区划内，考虑可能将姓名误填成籍贯。
   
   ​        对第9、第212数据，我们将其修改正确后重新投入数据集，而最后三条数据由于缺乏其它可以佐证的信息，无法推断修正籍贯，故在籍贯分类器设计里将其去除。最终，我们获得了拥有434条数据，以省为元素的标准籍贯数据集。其中各省份统计如下表所示：
   
   | 籍贯 | 数量 |  籍贯  | 数量 |
   | :--: | :--: | :----: | :--: |
   | 四川 | 129  |  甘肃  |  10  |
   | 河南 |  36  |  广东  |  9   |
   | 江西 |  26  |  贵州  |  9   |
   | 重庆 |  24  |  江苏  |  7   |
   | 安徽 |  24  |  广西  |  6   |
   | 山东 |  22  | 内蒙古 |  5   |
   | 湖北 |  20  |  吉林  |  3   |
   | 湖南 |  15  |  新疆  |  3   |
   | 陕西 |  14  |  辽宁  |  3   |
   | 河北 |  14  | 黑龙江 |  3   |
   | 福建 |  13  |  天津  |  2   |
   | 山西 |  13  |  北京  |  1   |
   | 浙江 |  11  |  海南  |  1   |
   | 云南 |  11  |        |      |

#### 地理信息划分

##### 1. 按省份划分

从上一步的表格信息可以看出，我们的数据集分散分布在27个省份，且各个省份之间差距较大，显然直接按省份分类是不合理的。按照基本的理解，我们首先想到的就是去掉部分数量很小的省份。

小于10的省份并合并成其它，将生成的标准数据图进行数据统计，可以统计出各个省级单位的人员分布情况，如下图表所示：

![pie](.\pie.png)

从图上看出，虽然数据较之前密集了，但依然存在很多的类别。这种方式训练的决策树对判断是不利的，从下文的结果也可以佐证我们的猜想。

##### 2. 按地理分区划分

既然直接省份划分不太合理，那我们就需要寻找一种合适的分类方式来对分类输出进行归并，解决输出的稀疏性问题。这里我参照[百度词条：中国地理区划](https://baike.baidu.com/item/%E4%B8%AD%E5%9B%BD%E5%9C%B0%E7%90%86%E5%8C%BA%E5%88%92/4221764?fr=aladdin)，将省份信息进行划分归组。

###### 地理分区

| 地区名称 |                             范围                             |
| :------: | :----------------------------------------------------------: |
| 北方地区 | 北京、天津、河北、山西、陕西、河南、山东 、黑龙江、吉林、辽宁 |
| 南方地区 | 江苏、安徽、浙江、上海、湖北、湖南、江西、福建、云南、贵州、四川、重庆、陕西、广西、广东、香港、澳门、海南、台湾、河南 |
| 西北地区 |                   内蒙古、新疆、宁夏、甘肃                   |
| 青藏地区 |                          青海、西藏                          |

![pie (3)](.\pie (3).png)

###### 行政分区

| 地区名称 |                             范围                             |
| :------: | :----------------------------------------------------------: |
|   华北   |         北京市、天津市、河北省、山西省、内蒙古自治区         |
|   东北   |                   黑龙江省、吉林省、辽宁省                   |
|   华东   | 上海市、江苏省、浙江省、安徽省、江西省、山东省、福建省，以及台湾省 |
|   华中   |                    河南省、湖北省、湖南省                    |
|   华南   | 广东省、广西壮族自治区、海南省，以及香港特别行政区、澳门特别行政区 |
|   西南   |          重庆市、四川省、贵州省、云南省、西藏自治区          |
|   西北   | 陕西省、甘肃省、青海省、宁夏回族自治区、新疆维吾尔自治区、内蒙古自治区 |

![pie (2)](.\pie (2).png)

###### 经济分区

| 地区名称 |                             范围                             |
| :------: | :----------------------------------------------------------: |
|   东部   | 北京、天津、河北、辽宁、上海、江苏、浙江、福建、山东、广东、广西、海南 |
|   中部   |   山西、内蒙古、吉林、黑龙江、安徽，江西、河南、湖北、湖南   |
|   西部   |  重庆、四川、贵州、云南、西藏、陕西、甘肃、青海、宁夏、新疆  |

![pie (4)](.\pie (4).png)

### 生成决策树

按照不同形式进行省份划分后，我们可以根据各个形式划分的方式进行决策树的搜索生成，并对比结果。我们利用sklearn官方库的寻优迭代器进行自动调参，并以5*5交叉验证后的平均准确率作为评估指标。下面是我们实验的结果。

#### 按省份划分

寻优调参后，最大得分划分结果如下，以省份划分之后检测平均准确率为25%：

![tree_loc (2)](.\tree_loc (2).png)

#### 按地理分区划分

寻优调参后，最大得分划分结果如下，以省份划分之后检测平均准确率为65%：

![tree_loc (3)](.\tree_loc (3).png)



#### 按行政分区划分

寻优调参后，最大得分划分结果如下，以地理分区划分之后检测平均准确率为40%：

![tree_loc (4)](.\tree_loc (4).png)



#### 按经济分区划分

寻优调参后，最大得分划分结果如下，以地理分区划分之后检测平均准确率为35%：

![tree_loc (5)](.\tree_loc (5).png)



#### 对比分析

经过前面对籍贯进行不同形式的归并，并统计结果统计后，我们发现将省份以地理分区归并对决策树的准确率提升较为明显。一开始，我们考虑到可能是由于分区较少导致对决策树准确率的提升。但在同样较少分区的经济分区划分方式里，决策树的平均准确率却只有地理分区的一半。最终我们猜测，可能是地理分区的籍贯划分方式符合我们数据集的潜藏特征分布，换句话说，也许按照地理分区的划分能够更明显的由数据集里的特征找到对应关联。

### 改进算法

#### 随机森林

综上，虽然按照地理分区的划分方式我们大大提高了准确率，但还有没有方法提升我们的模型效果呢？

经过研究，我们决定再尝试Bagging里最典型的随机森林算法，用随即森林对决策树进行改进并与原始决策树进行评估，最终10次交叉验证后的准确率对比如下图：

![compare](.\compare.png)

可以看出，使用随机森林算法对分类效果进一步产生了提升。经过比较，平均准确率相比决策树提高了约5%，这说明我们的改进是有意义的。

# 参考文献

https://scikit-learn.org/stable/modules/tree.html

https://blog.csdn.net/fuqiuai/article/details/79496005

https://zhuanlan.zhihu.com/p/123003914

https://stats.stackexchange.com/questions/105760/how-we-can-draw-an-roc-curve-for-decision-trees

https://www.cnblogs.com/ycycn/p/14063840.html

https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html#sphx-glr-auto-examples-tree-plot-cost-complexity-pruning-py

https://www.cnblogs.com/genyuan/p/9828457.html

https://blog.csdn.net/luteresa/article/details/104927276

https://blog.csdn.net/R18830287035/article/details/89257857

